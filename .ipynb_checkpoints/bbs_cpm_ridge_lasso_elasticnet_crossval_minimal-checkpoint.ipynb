{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T17:56:57.496449Z",
     "start_time": "2020-02-28T17:56:57.486640Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:49:43.572309Z",
     "start_time": "2020-02-27T21:49:43.566414Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats, linalg\n",
    "from sklearn import preprocessing, decomposition, linear_model, metrics \n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:46:55.631289Z",
     "start_time": "2020-02-27T20:46:54.423706Z"
    }
   },
   "outputs": [],
   "source": [
    "HCP_T1 = np.load('/net/parasite/HCP/Subjects/saige_temp_HCP/resampled/HCP_T1_array.npy')\n",
    "HCP_taskv = np.load('/net/parasite/HCP/Subjects/saige_temp_HCP/resampled/HCP_taskv_array.npy')\n",
    "HCP_taskc = np.load('/net/parasite/HCP/Subjects/saige_temp_HCP/resampled/HCP_taskc_array.npy')\n",
    "HCP_ages = np.load('/net/parasite/HCP/Subjects/saige_temp_HCP/resampled/age_labels.npy')\n",
    "HCP_g = np.load('/net/parasite/HCP/Subjects/saige_temp_HCP/resampled/g_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:46:55.650333Z",
     "start_time": "2020-02-27T20:46:55.638448Z"
    }
   },
   "outputs": [],
   "source": [
    "print(HCP_T1.shape)\n",
    "print(HCP_taskv.shape)\n",
    "print(HCP_taskc.shape)\n",
    "print(HCP_ages.shape)\n",
    "print(HCP_g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PNC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:46:55.631289Z",
     "start_time": "2020-02-27T20:46:54.423706Z"
    }
   },
   "outputs": [],
   "source": [
    "PNC_T1 = np.load('/net/pepper/Penn/PNC_T1_array.npy')\n",
    "PNC_taskv = np.load('/net/pepper/Penn/PNC_taskv_array.npy')\n",
    "PNC_taskc = np.load('/net/pepper/Penn/PNC_taskc_array.npy')\n",
    "PNC_ages = np.load('/net/pepper/Penn/PNC_age_labels.npy')\n",
    "PNC_g = np.load('/net/pepper/Penn/PNC_g_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:46:55.650333Z",
     "start_time": "2020-02-27T20:46:55.638448Z"
    }
   },
   "outputs": [],
   "source": [
    "print(PNC_T1.shape)\n",
    "print(PNC_taskv.shape)\n",
    "print(PNC_taskc.shape)\n",
    "print(PNC_ages.shape)\n",
    "print(PNC_g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:30.160969Z",
     "start_time": "2020-02-28T18:06:30.150093Z"
    }
   },
   "outputs": [],
   "source": [
    "HCP_T1_flat = HCP_T1.reshape((HCP_T1.shape[0], HCP_T1.shape[1]*HCP_T1.shape[2]*HCP_T1.shape[3]))  # HCP_T1_flat: (n_subjects, n_voxels)\n",
    "print(HCP_T1_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:30.160969Z",
     "start_time": "2020-02-28T18:06:30.150093Z"
    }
   },
   "outputs": [],
   "source": [
    "HCP_taskv_flat = HCP_taskv.reshape((HCP_taskv.shape[0], HCP_taskv.shape[1]*HCP_taskv.shape[2]*HCP_taskv.shape[3]))  # HCP_taskv_flat: (n_subjects, n_voxels)\n",
    "print(HCP_taskv_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:30.160969Z",
     "start_time": "2020-02-28T18:06:30.150093Z"
    }
   },
   "outputs": [],
   "source": [
    "PNC_T1_flat = PNC_T1.reshape((PNC_T1.shape[0], PNC_T1.shape[1]*PNC_T1.shape[2]*PNC_T1.shape[3]))  # PNC_T1_flat: (n_subjects, n_voxels)\n",
    "print(PNC_T1_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:30.160969Z",
     "start_time": "2020-02-28T18:06:30.150093Z"
    }
   },
   "outputs": [],
   "source": [
    "PNC_taskv_flat = PNC_taskv.reshape((PNC_taskv.shape[0], PNC_taskv.shape[1]*PNC_taskv.shape[2]*PNC_taskv.shape[3]))  # PNC_taskv_flat: (n_subjects, n_voxels)\n",
    "print(PNC_taskv_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:31.317947Z",
     "start_time": "2020-02-28T18:06:31.286075Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate HCP train/test splits\n",
    "np.random.seed(42)\n",
    "HCP_train = int(0.8 * HCP_T1_flat.shape[0])\n",
    "\n",
    "HCP_train_idxs = np.random.choice(range(HCP_T1_flat.shape[0]), size=HCP_train, replace=False)\n",
    "HCP_test_idxs = np.array([x for x in range(HCP_T1_flat.shape[0]) if x not in HCP_train_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:27:03.079155Z",
     "start_time": "2020-02-28T19:27:02.301318Z"
    }
   },
   "outputs": [],
   "source": [
    "train_HCP_T1 = HCP_T1_flat[HCP_train_idxs, :]\n",
    "test_HCP_T1 = HCP_T1_flat[HCP_test_idxs, :]\n",
    "\n",
    "train_HCP_taskv = HCP_taskv_flat[HCP_train_idxs, :]\n",
    "test_HCP_taskv = HCP_taskv_flat[HCP_test_idxs, :]\n",
    "\n",
    "train_HCP_taskc = HCP_taskc[HCP_train_idxs, :]\n",
    "test_HCP_taskc = HCP_taskc[HCP_test_idxs, :]\n",
    "\n",
    "train_HCP_phen = HCP_ages[HCP_train_idxs]\n",
    "test_HCP_phen = HCP_ages[HCP_test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:31.317947Z",
     "start_time": "2020-02-28T18:06:31.286075Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate PNC train/test splits\n",
    "np.random.seed(42)\n",
    "PNC_train = int(0.8 * PNC_T1_flat.shape[0])\n",
    "\n",
    "PNC_train_idxs = np.random.choice(range(PNC_T1_flat.shape[0]), size=PNC_train, replace=False)\n",
    "PNC_test_idxs = np.array([x for x in range(PNC_T1_flat.shape[0]) if x not in PNC_train_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:27:03.079155Z",
     "start_time": "2020-02-28T19:27:02.301318Z"
    }
   },
   "outputs": [],
   "source": [
    "train_PNC_T1 = PNC_T1_flat[PNC_train_idxs, :]\n",
    "test_PNC_T1 = PNC_T1_flat[PNC_test_idxs, :]\n",
    "\n",
    "train_PNC_taskv = PNC_taskv_flat[PNC_train_idxs, :]\n",
    "test_PNC_taskv = PNC_taskv_flat[PNC_test_idxs, :]\n",
    "\n",
    "train_PNC_taskc = PNC_taskc[PNC_train_idxs, :]\n",
    "test_PNC_taskc = PNC_taskc[PNC_test_idxs, :]\n",
    "\n",
    "train_PNC_phen = PNC_ages[PNC_train_idxs]\n",
    "test_PNC_phen = PNC_ages[PNC_test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Regression (BBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HCP T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:22.849443Z",
     "start_time": "2020-02-27T20:47:03.933802Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_model = decomposition.PCA(n_components=75).fit(train_HCP_T1)\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:25.521344Z",
     "start_time": "2020-02-27T20:47:23.812540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_HCP_T1_transformed = pca_model.transform(train_HCP_T1)\n",
    "test_HCP_T1_transformed = pca_model.transform(test_HCP_T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:52.511013Z",
     "start_time": "2020-02-28T21:02:52.496799Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLS using sklearn\n",
    "\n",
    "bbs_lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "bbs_lr_model.fit(train_HCP_T1_transformed, train_HCP_phen)\n",
    "train_preds_bbs_model = bbs_lr_model.predict(train_HCP_T1_transformed)\n",
    "test_preds_bbs_model = bbs_lr_model.predict(test_HCP_T1_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:03:07.671439Z",
     "start_time": "2020-02-28T21:03:07.658887Z"
    }
   },
   "outputs": [],
   "source": [
    "train_r2 = metrics.r2_score(train_HCP_phen, train_preds_bbs_model)\n",
    "train_mae = metrics.mean_absolute_error(train_HCP_phen, train_preds_bbs_model)\n",
    "test_mae = metrics.mean_absolute_error(test_HCP_phen, test_preds_bbs_model)\n",
    "\n",
    "print(f'Train R^2: {train_r2:.3f}')\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HCP Task (volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:22.849443Z",
     "start_time": "2020-02-27T20:47:03.933802Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_model = decomposition.PCA(n_components=75).fit(train_HCP_taskv)\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:25.521344Z",
     "start_time": "2020-02-27T20:47:23.812540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_HCP_taskv_transformed = pca_model.transform(train_HCP_taskv)\n",
    "test_HCP_taskv_transformed = pca_model.transform(test_HCP_taskv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:52.511013Z",
     "start_time": "2020-02-28T21:02:52.496799Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLS using sklearn\n",
    "\n",
    "bbs_lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "bbs_lr_model.fit(train_HCP_taskv_transformed, train_HCP_phen)\n",
    "train_preds_bbs_model = bbs_lr_model.predict(train_HCP_taskv_transformed)\n",
    "test_preds_bbs_model = bbs_lr_model.predict(test_HCP_taskv_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:03:07.671439Z",
     "start_time": "2020-02-28T21:03:07.658887Z"
    }
   },
   "outputs": [],
   "source": [
    "train_r2 = metrics.r2_score(train_HCP_phen, train_preds_bbs_model)\n",
    "train_mae = metrics.mean_absolute_error(train_HCP_phen, train_preds_bbs_model)\n",
    "test_mae = metrics.mean_absolute_error(test_HCP_phen, test_preds_bbs_model)\n",
    "\n",
    "print(f'Train R^2: {train_r2:.3f}')\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HCP Task (CIFTI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:22.849443Z",
     "start_time": "2020-02-27T20:47:03.933802Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_model = decomposition.PCA(n_components=75).fit(train_HCP_taskc)\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:25.521344Z",
     "start_time": "2020-02-27T20:47:23.812540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_HCP_taskc_transformed = pca_model.transform(train_HCP_taskc)\n",
    "test_HCP_taskc_transformed = pca_model.transform(test_HCP_taskc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:52.511013Z",
     "start_time": "2020-02-28T21:02:52.496799Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLS using sklearn\n",
    "\n",
    "bbs_lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "bbs_lr_model.fit(train_HCP_taskv_transformed, train_HCP_phen)\n",
    "train_preds_bbs_model = bbs_lr_model.predict(train_HCP_taskc_transformed)\n",
    "test_preds_bbs_model = bbs_lr_model.predict(test_HCP_taskc_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:03:07.671439Z",
     "start_time": "2020-02-28T21:03:07.658887Z"
    }
   },
   "outputs": [],
   "source": [
    "train_r2 = metrics.r2_score(train_HCP_phen, train_preds_bbs_model)\n",
    "train_mae = metrics.mean_absolute_error(train_HCP_phen, train_preds_bbs_model)\n",
    "test_mae = metrics.mean_absolute_error(test_HCP_phen, test_preds_bbs_model)\n",
    "\n",
    "print(f'Train R^2: {train_r2:.3f}')\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNC T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:22.849443Z",
     "start_time": "2020-02-27T20:47:03.933802Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_model = decomposition.PCA(n_components=75).fit(train_PNC_T1)\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:25.521344Z",
     "start_time": "2020-02-27T20:47:23.812540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_PNC_T1_transformed = pca_model.transform(train_PNC_T1)\n",
    "test_PNC_T1_transformed = pca_model.transform(test_PNC_T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:52.511013Z",
     "start_time": "2020-02-28T21:02:52.496799Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLS using sklearn\n",
    "\n",
    "bbs_lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "bbs_lr_model.fit(train_PNC_T1_transformed, train_PNC_phen)\n",
    "train_preds_bbs_model = bbs_lr_model.predict(train_PNC_T1_transformed)\n",
    "test_preds_bbs_model = bbs_lr_model.predict(test_PNC_T1_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:03:07.671439Z",
     "start_time": "2020-02-28T21:03:07.658887Z"
    }
   },
   "outputs": [],
   "source": [
    "train_r2 = metrics.r2_score(train_PNC_phen, train_preds_bbs_model)\n",
    "train_mae = metrics.mean_absolute_error(train_PNC_phen, train_preds_bbs_model)\n",
    "test_mae = metrics.mean_absolute_error(test_PNC_phen, test_preds_bbs_model)\n",
    "\n",
    "print(f'Train R^2: {train_r2:.3f}')\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNC Task (volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:22.849443Z",
     "start_time": "2020-02-27T20:47:03.933802Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_model = decomposition.PCA(n_components=75).fit(train_PNC_taskv)\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:25.521344Z",
     "start_time": "2020-02-27T20:47:23.812540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_PNC_taskv_transformed = pca_model.transform(train_PNC_taskv)\n",
    "test_PNC_taskv_transformed = pca_model.transform(test_PNC_taskv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:52.511013Z",
     "start_time": "2020-02-28T21:02:52.496799Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLS using sklearn\n",
    "\n",
    "bbs_lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "bbs_lr_model.fit(train_HCP_taskv_transformed, train_PNC_phen)\n",
    "train_preds_bbs_model = bbs_lr_model.predict(train_PNC_taskv_transformed)\n",
    "test_preds_bbs_model = bbs_lr_model.predict(test_PNC_taskv_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:03:07.671439Z",
     "start_time": "2020-02-28T21:03:07.658887Z"
    }
   },
   "outputs": [],
   "source": [
    "train_r2 = metrics.r2_score(train_PNC_phen, train_preds_bbs_model)\n",
    "train_mae = metrics.mean_absolute_error(train_PNC_phen, train_preds_bbs_model)\n",
    "test_mae = metrics.mean_absolute_error(test_HCP_phen, test_preds_bbs_model)\n",
    "\n",
    "print(f'Train R^2: {train_r2:.3f}')\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNC Task (CIFTI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:22.849443Z",
     "start_time": "2020-02-27T20:47:03.933802Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_model = decomposition.PCA(n_components=75).fit(train_PNC_taskc)\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:25.521344Z",
     "start_time": "2020-02-27T20:47:23.812540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_PNC_taskc_transformed = pca_model.transform(train_PNC_taskc)\n",
    "test_PNC_taskc_transformed = pca_model.transform(test_PNC_taskc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:52.511013Z",
     "start_time": "2020-02-28T21:02:52.496799Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLS using sklearn\n",
    "\n",
    "bbs_lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "bbs_lr_model.fit(train_PNC_taskv_transformed, train_PNC_phen)\n",
    "train_preds_bbs_model = bbs_lr_model.predict(train_PNC_taskc_transformed)\n",
    "test_preds_bbs_model = bbs_lr_model.predict(test_PNC_taskc_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:03:07.671439Z",
     "start_time": "2020-02-28T21:03:07.658887Z"
    }
   },
   "outputs": [],
   "source": [
    "train_r2 = metrics.r2_score(train_PNC_phen, train_preds_bbs_model)\n",
    "train_mae = metrics.mean_absolute_error(train_PNC_phen, train_preds_bbs_model)\n",
    "test_mae = metrics.mean_absolute_error(test_PNC_phen, test_preds_bbs_model)\n",
    "\n",
    "print(f'Train R^2: {train_r2:.3f}')\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectome Predictive Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:24:31.288665Z",
     "start_time": "2020-02-27T21:23:55.467812Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# correlation train_brain with train_phenotype\n",
    "with warnings.catch_warnings():\n",
    "    # we expect pearsonr to throw PearsonRConstantInputWarning because of contant valued columns in X\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    train_pheno_corr_p = [stats.pearsonr(train_T1[:, i], train_phen) for i in range(train_T1.shape[1])]  # train_pheno_corr_p: (259200, )\n",
    "    # there are some nan correlations if brain data is poorly cropped (ie: some columns are always 0)\n",
    "    \n",
    "    # split into positive and negative correlations \n",
    "    # and keep edges with p values below threshold\n",
    "    pval_threshold = 0.01\n",
    "\n",
    "    train_corrs = np.array([x[0] for x in train_pheno_corr_p])\n",
    "    train_pvals = np.array([x[1] for x in train_pheno_corr_p])\n",
    "\n",
    "    keep_edges_pos = (train_corrs > 0) & (train_pvals < pval_threshold)\n",
    "    keep_edges_neg = (train_corrs < 0) & (train_pvals < pval_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:29:03.616394Z",
     "start_time": "2020-02-27T21:29:03.571004Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pos_edges_sum = train_T1[:, keep_edges_pos].sum(1)\n",
    "train_neg_edges_sum = train_T1[:, keep_edges_neg].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:30:46.258668Z",
     "start_time": "2020-02-27T21:30:46.190775Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_pos = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(train_pos_edges_sum.reshape(-1, 1), train_phen)\n",
    "fit_neg = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(train_neg_edges_sum.reshape(-1, 1), train_phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:33:41.567137Z",
     "start_time": "2020-02-27T21:33:41.557150Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_error = metrics.mean_absolute_error(train_phen, fit_pos.predict(train_pos_edges_sum.reshape(-1, 1)))\n",
    "neg_error = metrics.mean_absolute_error(train_phen, fit_neg.predict(train_neg_edges_sum.reshape(-1, 1)))\n",
    "\n",
    "print(f'Training Error (Positive Edges Model) = {pos_error:.3f}')\n",
    "print(f'Training Error (Negative Edges Model) = {neg_error:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:34:05.962843Z",
     "start_time": "2020-02-27T21:34:05.952234Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine positive/negative edges in one linear regression model\n",
    "fit_pos_neg = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(np.stack((train_pos_edges_sum, train_neg_edges_sum)).T, train_phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:34:40.010030Z",
     "start_time": "2020-02-27T21:34:39.990469Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_neg_error = metrics.mean_absolute_error(train_phen, fit_pos_neg.predict(np.stack((train_pos_edges_sum, train_neg_edges_sum)).T))\n",
    "\n",
    "print(f'Training Error (Positive/Negative Edges Model) = {pos_neg_error:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:36:11.826718Z",
     "start_time": "2020-02-27T21:36:11.807247Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate out of sample performance \n",
    "test_pos_edges_sum = test_T1[:, keep_edges_pos].sum(1)\n",
    "test_neg_edges_sum = test_T1[:, keep_edges_neg].sum(1)\n",
    "\n",
    "pos_test_error = metrics.mean_absolute_error(test_phen, fit_pos.predict(test_pos_edges_sum.reshape(-1, 1)))\n",
    "neg_test_error = metrics.mean_absolute_error(test_phen, fit_neg.predict(test_neg_edges_sum.reshape(-1, 1)))\n",
    "pos_neg_test_error = metrics.mean_absolute_error(test_phen, fit_pos_neg.predict(np.stack((test_pos_edges_sum, test_neg_edges_sum)).T))\n",
    "\n",
    "print(f'Testing Error (Positive Edges Model) = {pos_test_error:.3f}')\n",
    "print(f'Testing Error (Negative Edges Model) = {neg_test_error:.3f}')\n",
    "print(f'Testing Error (Positive/Negative Edges Model) = {pos_neg_test_error:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso (Linear Regression + L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:57:14.623765Z",
     "start_time": "2020-02-28T18:52:40.754297Z"
    }
   },
   "outputs": [],
   "source": [
    "# LassoCV uses coordinate descent to select hyperparameter alpha \n",
    "alpha_grid = np.array([10**a for a in np.arange(-3, 3, 0.25)])\n",
    "lassoCV_model = linear_model.LassoCV(cv=5, n_alphas=len(alpha_grid), alphas=alpha_grid, fit_intercept=True, normalize=False, random_state=42, verbose=True, n_jobs=5).fit(train_T1, train_phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:03:05.522140Z",
     "start_time": "2020-02-28T19:03:00.140879Z"
    }
   },
   "outputs": [],
   "source": [
    "# based on cv results above, set alpha=100\n",
    "lasso_model = linear_model.Lasso(alpha=lassoCV_model.alpha_, fit_intercept=True, normalize=False).fit(train_T1, train_phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:26:16.703261Z",
     "start_time": "2020-02-28T19:26:16.247169Z"
    }
   },
   "outputs": [],
   "source": [
    "train_preds_lasso_model = lasso_model.predict(train_T1)\n",
    "test_preds_lasso_model = lasso_model.predict(test_T1)\n",
    "\n",
    "train_mae = metrics.mean_absolute_error(train_phen, train_preds_lasso_model)\n",
    "test_mae = metrics.mean_absolute_error(test_phen, test_preds_lasso_model)\n",
    "\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge (Linear Regression + L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:24:20.576475Z",
     "start_time": "2020-02-28T19:23:29.159220Z"
    }
   },
   "outputs": [],
   "source": [
    "# RidgeCV uses generalized cross validation to select hyperparameter alpha \n",
    "with warnings.catch_warnings():\n",
    "    # ignore matrix decomposition errors\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    ridgeCV_model = linear_model.RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, cv=5).fit(train_T1, train_phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:04:28.613450Z",
     "start_time": "2020-02-28T20:04:28.599216Z"
    }
   },
   "outputs": [],
   "source": [
    "ridge_alpha = ridgeCV_model.alpha_\n",
    "print(f'CV Selected Alpha = {ridge_alpha:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:04:35.576044Z",
     "start_time": "2020-02-28T20:04:29.018975Z"
    }
   },
   "outputs": [],
   "source": [
    "ridge_model = linear_model.Ridge(alpha=ridge_alpha, fit_intercept=True, normalize=False).fit(train_T1, train_phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:04:36.305177Z",
     "start_time": "2020-02-28T20:04:35.589009Z"
    }
   },
   "outputs": [],
   "source": [
    "train_preds_ridge_model = ridge_model.predict(train_T1)\n",
    "test_preds_ridge_model = ridge_model.predict(test_T1)\n",
    "\n",
    "train_mae = metrics.mean_absolute_error(train_phen, train_preds_ridge_model)\n",
    "test_mae = metrics.mean_absolute_error(test_phen, test_preds_ridge_model)\n",
    "\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net (Linear Regression + L1/L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:59:20.607271Z",
     "start_time": "2020-02-28T20:25:43.581804Z"
    }
   },
   "outputs": [],
   "source": [
    "# RidgeCV uses generalized cross validation to select hyperparameter alpha \n",
    "elasticnetCV_model = linear_model.ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=5, n_alphas=len(alpha_grid), alphas=alpha_grid, random_state=42, verbose=True, n_jobs=5).fit(train_T1, train_phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:59:20.620899Z",
     "start_time": "2020-02-28T20:59:20.612966Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'CV selected alpha {elasticnetCV_model.alpha_:.3f}')\n",
    "print(f'Elastic net L1 ratio {elasticnetCV_model.l1_ratio_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:35.399337Z",
     "start_time": "2020-02-28T21:02:30.121679Z"
    }
   },
   "outputs": [],
   "source": [
    "elasticnet_model = linear_model.ElasticNet(alpha=elasticnetCV_model.alpha_, l1_ratio=elasticnetCV_model.l1_ratio_, fit_intercept=True, normalize=False, random_state=42).fit(train_T1, train_phen)\n",
    "\n",
    "train_preds_en_model = elasticnet_model.predict(train_T1)\n",
    "test_preds_en_model = elasticnet_model.predict(test_T1)\n",
    "\n",
    "train_mae = metrics.mean_absolute_error(train_phen, train_preds_en_model)\n",
    "test_mae = metrics.mean_absolute_error(test_phen, test_preds_en_model)\n",
    "\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPGbaF4WFxf0"
   },
   "source": [
    "# **3-D CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y8r5IPOSFxf3",
    "outputId": "02465af3-8365-4ca9-83f1-db2e01e6f8e2"
   },
   "outputs": [],
   "source": [
    "all_subs_T1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnMCwGm-Fxf1"
   },
   "outputs": [],
   "source": [
    "all_subs_T1_CNN = all_subs_T1.reshape(-1,60,72,60,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybZz7sKWFxf6"
   },
   "outputs": [],
   "source": [
    "train_T1_CNN = all_subs_T1_CNN[train_idxs,:,:,:,:]\n",
    "test_T1_CNN = all_subs_T1_CNN[test_idxs,:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iQqecvStFxf9",
    "outputId": "0964e506-0889-4e92-d04b-17362bdb08bb"
   },
   "outputs": [],
   "source": [
    "train_T1_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7UOkb3cGFxf_",
    "outputId": "7bf2b49c-4bf3-46a8-c44d-70f46c3c0890"
   },
   "outputs": [],
   "source": [
    "test_T1_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCaeMbfsFxgC"
   },
   "outputs": [],
   "source": [
    "image_shape = train_T1_CNN[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "5rXDDliUFxgE",
    "outputId": "1f61b637-094c-464d-add1-fd3342be6ca1"
   },
   "outputs": [],
   "source": [
    "model = Sequential() # The simplest model, a linear stack of layers\n",
    "\n",
    "model.add(Conv3D(filters=64,\n",
    "                 kernel_size=(3,3,3), #determines the width, height, depth of the 3D convolution window\n",
    "                 activation='elu', #Exponential Linear Unit\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform', \n",
    "                 input_shape=image_shape)) # only the first layer needs to be told this info\n",
    "model.add(Conv3D(filters=64, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(Conv3D(filters=64, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(MaxPooling3D((2,2,2),strides=(2,2,2))) # pooling is also referred to as a downsampling layer\n",
    "model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch (aka make the mean activation close to 0 and the activation standard deviation close to 1)\n",
    "\n",
    "model.add(Conv3D(filters=32, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(Conv3D(filters=32, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv3D(filters=16, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(Conv3D(filters=16, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv3D(filters=8, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(Conv3D(filters=8, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(AveragePooling3D((2,2,2),strides=(2,2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu',name='features')) #convert the output of the convolutional part of the CNN into a 1D feature vector. Length of vector = n_classes\n",
    "model.add(Dense(1)) # final output is a single number (Age in this model)\n",
    "model.summary()\n",
    "\n",
    "filename=\"best_weights.h5\"\n",
    "filename2=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1MWZwDjFxgG"
   },
   "outputs": [],
   "source": [
    "checkpoints = []\n",
    "\n",
    "if not os.path.exists('3DCNN_Results_g/'):\n",
    "    os.makedirs('3DCNN_Results_g/')\n",
    "\n",
    "checkpoints.append(ModelCheckpoint('3DCNN_Results_g/'+filename, \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True, \n",
    "                                   save_weights_only=True, \n",
    "                                   mode='auto', \n",
    "                                   period=1))\n",
    "\n",
    "checkpoints.append(ModelCheckpoint('3DCNN_Results_g/'+filename2, \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=False, \n",
    "                                   save_weights_only=True, \n",
    "                                   mode='auto', \n",
    "                                   period=20))\n",
    "\n",
    "checkpoints.append(TensorBoard(log_dir='3DCNN_Results_g/TensorBoardLogs', \n",
    "                               histogram_freq=0, \n",
    "                               write_graph=True, \n",
    "                               write_images=False, \n",
    "                               embeddings_freq=0, \n",
    "                               embeddings_layer_names=['features'], \n",
    "                               embeddings_metadata='metadata.tsv'))\n",
    "#Early Stopping here is set so that if the MSE in the validation set does not improve after 10 epochs, training will stop\n",
    "checkpoints.append(EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=10))\n",
    "checkpoints.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "checkpoints.append(CSVLogger('3DCNN_Results_g/log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8-MFOaOFxgJ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', # the objective that the model will try to minimize, Mean Square Error in this model\n",
    "              optimizer='adam', \n",
    "              metrics=['mae']) # add in any other metrics you want to use to show performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "rVX5w2zHFxgL",
    "outputId": "59acdeea-27b2-41c3-8285-cfdc1e47e4c8"
   },
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "svhgREKsFxgN",
    "outputId": "14952ba2-bd7e-46e1-fe2e-6f2e067bf420"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 500 # defines for how many times the training will repeat. 1 epoch is 1 forward pass and 1 backward pass over all the training examples\n",
    "BATCH_SIZE = 20 # the number of training examples in one forward/backward pass (or for 1 epoch)\n",
    "history1 = model.fit(train_T1_CNN, train_phen, \n",
    "          validation_split = 0.1, # This sets how much of the training data should be used as the validation set (test set during training) \n",
    "          epochs = NUM_EPOCHS,\n",
    "          batch_size= BATCH_SIZE,\n",
    "          callbacks = checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "V_FbHQAqFxgP",
    "outputId": "24a86979-33cd-4898-e4b1-f9d1341eaf07"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x=test_T1_CNN, y=test_phen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2-D CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_taskv_CNN = PNC_taskv[PNC_train_idxs,:,:,:]\n",
    "test_taskv_CNN = PNC_taskv[PNC_test_idxs,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = train_taskv_CNN[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # The simplest model, a linear stack of layers\n",
    "\n",
    "model.add(Conv2D(filters=64,\n",
    "                 kernel_size=(3,3), #determines the width, height, depth of the 3D convolution window\n",
    "                 activation='elu', #Exponential Linear Unit\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform', \n",
    "                 input_shape=image_shape)) # only the first layer needs to be told this info\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2))) # pooling is also referred to as a downsampling layer\n",
    "model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch (aka make the mean activation close to 0 and the activation standard deviation close to 1)\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=8, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(Conv2D(filters=8, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(AveragePooling2D((2,2),strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu',name='features')) #convert the output of the convolutional part of the CNN into a 1D feature vector. Length of vector = n_classes\n",
    "model.add(Dense(1)) # final output is a single number (Age in this model)\n",
    "model.summary()\n",
    "\n",
    "filename=\"best_weights.h5\"\n",
    "filename2=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = []\n",
    "\n",
    "if not os.path.exists('2DCNN_Results_age_task/'):\n",
    "    os.makedirs('2DCNN_Results_age_task/')\n",
    "\n",
    "checkpoints.append(ModelCheckpoint('2DCNN_Results_age_task/'+filename, \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True, \n",
    "                                   save_weights_only=True, \n",
    "                                   mode='auto', \n",
    "                                   period=1))\n",
    "\n",
    "checkpoints.append(ModelCheckpoint('2DCNN_Results_age_task/'+filename2, \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=False, \n",
    "                                   save_weights_only=True, \n",
    "                                   mode='auto', \n",
    "                                   period=20))\n",
    "\n",
    "checkpoints.append(TensorBoard(log_dir='2DCNN_Results_age_task/TensorBoardLogs', \n",
    "                               histogram_freq=0, \n",
    "                               write_graph=True, \n",
    "                               write_images=False, \n",
    "                               embeddings_freq=0, \n",
    "                               embeddings_layer_names=['features'], \n",
    "                               embeddings_metadata='metadata.tsv'))\n",
    "#Early Stopping here is set so that if the MSE in the validation set does not improve after 10 epochs, training will stop\n",
    "checkpoints.append(EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=10))\n",
    "checkpoints.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "checkpoints.append(CSVLogger('2DCNN_Results_age_task/log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', # the objective that the model will try to minimize, Mean Square Error in this model\n",
    "              optimizer='adam', \n",
    "              metrics=['mae']) # add in any other metrics you want to use to show performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 500 # defines for how many times the training will repeat. 1 epoch is 1 forward pass and 1 backward pass over all the training examples\n",
    "BATCH_SIZE = 20 # the number of training examples in one forward/backward pass (or for 1 epoch)\n",
    "history1 = model.fit(train_taskv_CNN, train_PNC_phen, \n",
    "          validation_split = 0.1, # This sets how much of the training data should be used as the validation set (test set during training) \n",
    "          epochs = NUM_EPOCHS,\n",
    "          batch_size= BATCH_SIZE,\n",
    "          callbacks = checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=test_taskv_CNN, y=test_PNC_phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
